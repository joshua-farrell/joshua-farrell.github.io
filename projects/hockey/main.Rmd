---
title: "NHL Data Analysis"
output:
  html_document:
    df_print: paged
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(out.width = "60%")
knitr::opts_chunk$set(fig.align = "center")

source("./packages.R")
source("./shared.R")
source("./import.R")
source("./select.R")
source("./clean.R")
source("./train.R")
source("./graph.R")
```

## Data setup

Setup your working directory:

```{r}
setwd("C:/Users/farre/Desktop/nhl")
```

Import the data:

```{r}
data <- import("./data")
```

I'm not going very deep into what variables are polluted due to the amount of
time this has taken and doing so would take, but there are some variables that
are clearly polluted and their values should be checked. For example, `TOT_CHIP`
has values that have monetary representations but also others that don't, and
which don't seem to be in the same units of scale. See the following output to
get a better idea. When doing the data cleaning, this are all transformed into
numeric variables, but they are problematic and should be investigated further.

```{r}
unique(data[, "TOT_CHIP"])
```

The proportion of NAs in the data is very large. You can see the descriptive
statistics for the distribution across all observations and variables in the
following code. As you can see, the mean proportion of NAs per variable is 0.53
and the first quartile is already at 0.31, which is very high. For observations
the numbers are similarly bad. This will have very bad implications for the ML
models, so we need a mechanism to reduce the proportion of NAs while trying to
keep most of the useful information.

If you want to see proportions in a case-by-case (i.e. "by row" or "by column"),
you can remove the `summary()` function that wrapping any of it.

```{r}
summary(proportion_of_nas_per_observation(data))
summary(proportion_of_nas_per_variable(data))
```

To reduce the NA proportions in the data, we look for observations and variables
that have proportions larger than `NA_OBS_PROPORTION_THRESHOLD` and
`NA_VAR_PROPORTION_THRESHOLD`, respectively, and delete them if they do. In the
case of variables, there's a `NA_VAR_WHITELIST` variable that will keep any
variables inside it, even if they don't pass the NAs proportion test. All of
these constants can be modified in the `constants.R` file.

Since I'm not familiar with the data, I can't know which variables whould be
white-listed, but, if you do, you should definitely add them to the appropriate
parameter in the `cosntants.R` file.

```{r}
before_n_obs <- nrow(data)
before_n_vars <- ncol(data)
```

```{r}
data <- clean(data)
```

After we apply our cleaning process, the mean proportion of NAs for both
variables and observations goes down to 0.38 and we keep 44% of the variables
and 70% of the observations, which is not too bad at all given the huge
proportion of NAs in the data. Note that this also acts as an initial variable
selection procedure, and it can be relaxed or made more restrictive by modifying
the `NA_OBS_PROPORTION_THRESHOLD` and `NA_VAR_PROPORTION_THRESHOLD` in the
`constants.R` file.

```{r}
summary(proportion_of_nas_per_observation(data))
summary(proportion_of_nas_per_variable(data))

ncol(data) / before_n_vars
nrow(data) / before_n_obs
```

This is the clean and pre-processed data set, and it's saved for reference
(before teh variable selection process is applied), since it may be easier to
check for correctness outside of R.

```{r}
write.csv(data, "./data.csv", row.names = FALSE)
```

Now we are going to impute the mode to each NA in each categorical variable, and
the mean in the case of numerical variables. If we don't do this, the, sill very
large, number of NAs will produce errors in our variable selection and model
training processes. After the imputation, we separete the data into our train,
test, and full data sets

```{r}
data <- separate(impute(data))
```

## Visualizations

Salary distribution:

```{r}
options(scipen = 999)
hist(data[["full"]]$Salary, main = NULL, xlab = "Salary")
```

Since there's a very large number of numeric variables, we can't generate a
single correlations plots that has all the information simultaneously. We show
the correlations by variable random samples, which means that you should execute
the following code multiple times to get a rough estimate of the correlations
among all variables.

```{r, fig.height = 10, fig.width = 10, out.width = "100%"}
random_correlations_graph(data, n_variables = 20)
```

```{r, fig.height = 10, fig.width = 10, out.width = "100%"}
random_correlations_graph(data, n_variables = 20)
```

```{r, fig.height = 10, fig.width = 10, out.width = "100%"}
random_correlations_graph(data, n_variables = 20)
```

The following code automatically generate multiple correlations graphs and
saves them to disk for you. You can modify the parameters to your liking.
Values marked with a cross on top of them mean that their p-value is larger
than 0.01.

```r
generate_multiple_random_correlation_graphs(
    n_variables = 20,
    n_graphs = 10,
    data = data
)
```

We also are able to create scatter graphs where the size of each point is
determined a variable's value, and the color is determined by the salary. Some
examples of the different patterns that can be found with these graphs are the
following. Note that to interpret these correctly good domain knowledge about
NHL and the measured variables is required, but even if no such knowledge is
available, some interesting ideas can be infered for data cleaning and feature
engineering purposes. Like these, tere are many other interesting patterns
hidden in this data.

```{r, out.width = "100%"}
scatter_graph_with_size_and_salary_color(
    data,
    x = "ES_CF.QoC",
    y = "TOT_FOL.Close",
    size = "PK_CF.QoT"
)
```

```{r, out.width = "100%"}
scatter_graph_with_size_and_salary_color(
    data,
    x = "ES_CF.QoT",
    y = "PK_PDO",
    size = "PK_iHDf"
)
```

```{r, out.width = "100%"}
scatter_graph_with_size_and_salary_color(
    data,
    x = "ES_DZS.1",
    y = "PK_TOI.QoT",
    size = "ES_iCF"
)
```

```{r, out.width = "100%"}
scatter_graph_with_size_and_salary_color(
    data,
    x = "ES_F.Tied",
    y = "ES_C.Tied",
    size = "PP_iPEND"
)
```

```{r, out.width = "100%"}
scatter_graph_with_size_and_salary_color(
    data,
    x = "ES_FA",
    y = "ES_GF.1",
    size = "ES_F.Down"
)
```

```{r, out.width = "100%"}
scatter_graph_with_size_and_salary_color(
    data,
    x = "ES_iHA",
    y = "TOT_iHA",
    size = "ES_iSF.1"
)
```

To create `n_graphs` random graphs (the combinations for all available graphs
are too much to create at once, it would be around 10,827,401) we use the
following:

> **CAUTION**: The following code can take a long time to finish depending on
> how many graphs you specify. To avoid any problems, it's not activiated here and
> is just left as reference.

```r
generate_multiple_random_scatter_graphs(
    n_graphs = 100,
    data = data
)
```

The scatter graphs created show some very interesting behavior for many
combinations. Those visualizations together with domain knowledge around the
problem can help with cleaning and feature engineering to increase the
predictive power of the models, and it's something that I recommend you explor
further.

## Variable selection

Variable selection is performed with two steps. First, we remove variables which
are highly correlated with others. The logic behind this is that if they are
highly correlated the information they provide for the analysis is redundant,
and, given the large number of variables in the analysis, we want to minimize
the number of them while we maximize the information kept. The second step is
performed with Recursive Feature Elimination (RFE).

The `selection` object contains the following attributes: `data` which is the
data with the variable selection applied, `graph` which shows the accuracy
change as more variables are added, `n_variables` which is the number of
variables selected, and `variables` which are the variables actually selected.
Internally, Random Forests are used to measure variable importance.

```{r}
selection <- variable_selection(data, "Salary")
```

To explore the selection results, you can use the following to see the filtered
datasets, variables, and optimization graph.

```r
head(selection[["data"]][["full"]])
head(selection[["data"]][["train"]])
head(selection[["data"]][["test"]])
```

```{r}
selection[["n_variables"]]
selection[["variables"]]
```

```{r, out.width = "100%"}
selection[["graph"]]
```

## Model application

Feel free to add other models, you can find the full list here:
https://topepo.github.io/caret/available-models.html

These models depend on external packages. If you don't have installed in your
system, when you execute the code, you'll be notified that you don't have them,
and you'll be asked whether you want to install them. If you do, execution will
continue normally after the corresponding installations.

```{r}
models <- list(
    "random_forest" = "rf",
    "linear_regression" = "glm",
    "k_nearest_neighbors" = "knn"
)
```

```{r}
results <- best_results(selection[["data"]], "Salary", models)
```

## Results exploration

The `metrics` attribute has find three different accuracy metrics: `RMSE` (Root
Mean Squared Error), `MAPE` (Mean Absolute Percentage Error), and `PPE10`
(Percentage Predicted Error within 10%).

Models available:

```{r}
names(results)
```

Random Forest results:

```{r}
names(results[["random_forest"]])
```

See the `predictor` object for Random Forests:

```{r}
results[["random_forest"]][["predictor"]]
```

The results I'm seeing in average are the following. Note that these may be
different every time you run the analysis because we're not controlling the
"seed" for the randomized algorithmns (this is done on purpose to see variuos
results, but for publishing or sharing results with someone else, we should
control the seed).

| Model | Metric | Value |
|-------|--------|-------|
| k-Nearest Neighbors | MAPE | 0.73 |
| k-Nearest Neighbors | RMSE | 1,124,504 |
| k-Nearest Neighbors | PPE10 | 0.1843 |
| Linear Regression | MAPE | 0.82 |
| Linear Regression | RMSE | 1,029,118 |
| Linear Regression | PPE10 | 0.1099 |
| Random Forest | MAPE | 0.4722 |
| Random Forest | RMSE | 720,575 |
| Random Forest | PPE10 | 0.2553 |

The results without the `Team` variable are (may be different for you):

| Model | Metric | Value |
|-------|--------|-------|
| k-Nearest Neighbors | MAPE | 0.7182 |
| k-Nearest Neighbors | RMSE | 1,145,584 |
| k-Nearest Neighbors | PPE10 | 0.1276 |
| Linear Regression | MAPE | 0.7932 |
| Linear Regression | RMSE | 1,022,017 |
| Linear Regression | PPE10 | 0.1134 |
| Random Forest | MAPE | 0.4237 |
| Random Forest | RMSE | 759,186 |
| Random Forest | PPE10 | 0.2234 |

These results show that Linear Regression performs better when including the
`Team` variable, but the other two models perform worse. Note that these are the
results of a single run, and numbers are close enough that the samples used
during each run could have an impact on this result. For robustness, we would
have to run the analysis many times (without controlling the seed), and see what
we get in average.

Also note that these results show that the predictive powerful for these models
is not good. We could try more domain-specific feature engineering to increase
their predictive power, but that can take much time and is out of scope for now.
Also note the pending issue with the `TOT_Cap.Hit` and `TOT_NHLid` variables.

```{r}
results[["k_nearest_neighbors"]][["metrics"]]
results[["linear_regression"]][["metrics"]]
results[["random_forest"]][["metrics"]]
```

See the "variable" `importance`, for a Random Forests

```{r, fig.height = 10, out.width = "100%"}
results[["random_forest"]][["importance"]]
```

For "variable importance" for linear regression, you can look directly at the
coefficients, standard errors, and p-values from the fitte model

```{r}
summary(results[["linear_regression"]][["predictor"]]$finalModel)
```

## Predictions for specific player's salary

For one of Connor McDavid's observations we have a relative good predictions,
and, for the other, it seems to be very off. In other cases (which are random
and thus different on each execution), results seem to be much better. It seems
as if Connor McDavid is a difficult case for predictions.

Data we will use for predictions:

```{r}
select <- which(
    colnames(data[["full"]]) %in%
    c(colnames(selection[["data"]][["full"]]), ID_VARS)
)
d <- data[["full"]][, select]
```

Connor McDavid observations (note that there are two observations for him):

```{r}
connor_mcdavid <- d[
    d$Last.Name == "McDavid" &
    d$First.Name == "Connor" &
    d$Position == "C" &
    d$Team == "EDM",
]
```

Nine random players plus the two observatinos for Connor McDavid:

```{r}
players <- d[sample(1:nrow(d), 9), ]
players <- rbind(connor_mcdavid, players)
```

Get the actual predictions:

```{r}
avoid <- -which(colnames(players) %in% c(
    "Year",
    "Last.Name",
    "First.Name"
))
predictions <- predict(
    results[["random_forest"]][["predictor"]],
    players[, avoid]
)
```

Results for predictions, including Root-Square Error (RSE), Absolute Error (AE),
and Absolute Percentage Error (APE).

```{r}
res <- data.frame(
    "First.Name" = players$First.Name,
    "Last.Name" = players$Last.Name,
    "Position" = players$Position,
    "Team" = players$Team,
    "Real" = players$Salary,
    "Predicted" = predictions
)
res$RSE <- sqrt((res$Real - res$Predicted)^2)
res$AE <- abs(res$Real - res$Predicted)
res$APE <- res$AE / res$Real

res
```
